{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Imports and kaggle setup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:33:30.164969Z","iopub.status.busy":"2023-03-23T08:33:30.164595Z","iopub.status.idle":"2023-03-23T08:33:55.899618Z","shell.execute_reply":"2023-03-23T08:33:55.898297Z","shell.execute_reply.started":"2023-03-23T08:33:30.164915Z"},"id":"5D9wqRh9_kPE","trusted":true},"outputs":[],"source":["!pip install POT -q\n","!pip install wandb -q\n","\n","from torchvision import datasets\n","import torchvision.datasets.utils as dataset_utils\n","import torchvision\n","\n","from ot.bregman import sinkhorn\n","from ot.lp import emd\n","\n","from ignite.metrics import SSIM, PSNR\n","\n","import yaml\n","import sys\n","import os\n","\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","import cv2\n","import numpy as np\n","import os\n","import random\n","from torchvision.transforms import ToTensor\n","from torch.utils.data import Dataset, DataLoader\n","\n","from functools import partial\n","\n","import torch\n","import torch.nn as nn\n","\n","from abc import abstractmethod\n","import math\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from tqdm import tqdm\n","import copy\n","\n","import wandb\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","torch.manual_seed(1234)\n","random.seed(1234)\n","np.random.seed(1234)\n","torch.cuda.manual_seed(1234)\n","\n","torch.backends.cudnn.deterministic=True\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","#for metrics\n","from ignite.engine import *\n","from ignite.handlers import *\n","from ignite.metrics import *\n","from ignite.utils import *\n","from ignite.contrib.metrics.regression import *\n","from ignite.contrib.metrics import *\n","\n","def eval_step(engine, batch):\n","    return batch\n","\n","default_evaluator = Engine(eval_step)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:33:55.904151Z","iopub.status.busy":"2023-03-23T08:33:55.903821Z","iopub.status.idle":"2023-03-23T08:33:56.880122Z","shell.execute_reply":"2023-03-23T08:33:56.878655Z","shell.execute_reply.started":"2023-03-23T08:33:55.904117Z"},"trusted":true},"outputs":[],"source":["ls"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%writefile kaggle.json\n","{\"username\":\"kaggle3223\",\"key\":\"de631c18f08a192190f69129affaaa2e\"}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!mkdir ~/.kaggle/\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:34:20.649270Z","iopub.status.busy":"2023-03-23T08:34:20.648770Z","iopub.status.idle":"2023-03-23T08:34:20.662150Z","shell.execute_reply":"2023-03-23T08:34:20.660120Z","shell.execute_reply.started":"2023-03-23T08:34:20.649221Z"},"trusted":true},"outputs":[],"source":["device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:34:21.634753Z","iopub.status.busy":"2023-03-23T08:34:21.634251Z","iopub.status.idle":"2023-03-23T08:34:26.802934Z","shell.execute_reply":"2023-03-23T08:34:26.801641Z","shell.execute_reply.started":"2023-03-23T08:34:21.634705Z"},"id":"kdmwLk_UE1z2","trusted":true},"outputs":[],"source":["# !wandb login"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Create config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:34:34.645347Z","iopub.status.busy":"2023-03-23T08:34:34.644297Z","iopub.status.idle":"2023-03-23T08:34:34.653807Z","shell.execute_reply":"2023-03-23T08:34:34.652566Z","shell.execute_reply.started":"2023-03-23T08:34:34.645303Z"},"id":"hjazGUsdARiH","outputId":"e8c9d3ef-6cb1-41f0-ad6e-017473bc13f3","trusted":true},"outputs":[],"source":["%%writefile conf.yml\n","\n","MODE : 2   # 1 Train, 2 Metrics\n","IMAGE_SIZE : [32, 32] #[224,224]\n","CHANNEL_X : 3\n","CHANNEL_Y : 3\n","TIMESTEPS : 1000 #2000\n","MODEL_CHANNELS : 128\n","NUM_RESBLOCKS : 4\n","ATTENTION_RESOLUTIONS : [2,4,8]\n","DROPOUT : 0\n","CHANNEL_MULT : [1,2,4,8]\n","CONV_RESAMPLE : 'True'\n","USE_CHECKPOINT : 'False'\n","USE_FP16 : 'False'\n","NUM_HEADS : 1\n","NUM_HEAD_CHANNELS : 64\n","NUM_HEAD_UPSAMPLE : -1\n","USE_SCALE_SHIFT_NORM : 'False'\n","RESBLOCK_UPDOWN : 'False'\n","USE_NEW_ATTENTION_ORDER : 'False'\n","PATH_COLOR : \"./ab/ab/ab1.npy\" #'color'\n","PATH_GREY : \"./l/gray_scale.npy\"  #'grayscale'\n","BATCH_SIZE : 64\n","BATCH_SIZE_VAL : 8\n","ITERATION_MAX : 20000\n","LR : 0.0001\n","LOSS : 'L2'\n","VALIDATION_EVERY : 1000\n","EMA_EVERY : 100\n","START_EMA : 2000\n","SAVE_MODEL_EVERY : 5000 #10000\n","PLOT_EVERY : 50\n","EXP_NAME: \"shoes_bags_fixed_unregularized\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:34:46.156825Z","iopub.status.busy":"2023-03-23T08:34:46.155872Z","iopub.status.idle":"2023-03-23T08:34:46.165054Z","shell.execute_reply":"2023-03-23T08:34:46.163965Z","shell.execute_reply.started":"2023-03-23T08:34:46.156787Z"},"id":"vfNOOoVxZwex","trusted":true},"outputs":[],"source":["class Config(dict):\n","    def __init__(self, config_path):\n","        with open(config_path, 'r') as f:\n","            self._yaml = f.read()\n","            self._dict = yaml.safe_load(self._yaml)\n","            self._dict['PATH'] = os.path.dirname(config_path)\n","\n","    def __getattr__(self, name):\n","        if self._dict.get(name) is not None:\n","            return self._dict[name]\n","        return None\n","\n","    def print(self):\n","        print('Model configurations:')\n","        print('---------------------------------')\n","        print(self._yaml)\n","        print('')\n","        print('---------------------------------')\n","        print('')\n","\n","\n","def load_config(path):\n","    \n","    config_path = path\n","    config = Config(config_path)\n","    return(config)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Load data, create custom dataset with OT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:34:48.282232Z","iopub.status.busy":"2023-03-23T08:34:48.281198Z","iopub.status.idle":"2023-03-23T08:35:13.118462Z","shell.execute_reply":"2023-03-23T08:35:13.117070Z","shell.execute_reply.started":"2023-03-23T08:34:48.282185Z"},"trusted":true},"outputs":[],"source":["# for shoes handbags\n","!FILEID='1i1F462P45I2w3lIFL-u8gwmcPm3iEAGb' && \\\n","FILENAME='shoes_bags_data.zip' && \\\n","FILEDEST=\"https://docs.google.com/uc?export=download&id=${FILEID}\" && \\\n","wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate ${FILEDEST} -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=${FILEID}\" -O $FILENAME && rm -rf /tmp/cookies.txt\n","\n","!unzip shoes_bags_data.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:35:13.121306Z","iopub.status.busy":"2023-03-23T08:35:13.120953Z","iopub.status.idle":"2023-03-23T08:35:13.131765Z","shell.execute_reply":"2023-03-23T08:35:13.130106Z","shell.execute_reply.started":"2023-03-23T08:35:13.121269Z"},"trusted":true},"outputs":[],"source":["def load_shoes_bags(data_path=\"./shoes_tensor_32.torch\",total_size=11000):\n","    data = torch.load(data_path)\n","    \n","    random_idx = np.random.choice(np.arange(len(data)), size=total_size, replace=False)\n","    data = data[random_idx]\n","    \n","    np.random.shuffle(random_idx)\n","    train_data, test_data = data[:10000, :], data[10000:, :]\n","    assert len(train_data) == total_size - 1000\n","    return train_data, test_data\n","\n","\n","class ShoeBagDataset(Dataset):\n","    def __init__(self, data_from, data_to):\n","        self.data_from = data_from\n","        self.data_to = data_to\n","\n","    def __len__(self):\n","        return len(self.data_from)\n","\n","    def __getitem__(self, idx: int):\n","        \"\"\"\n","            returns single sample\n","        \"\"\"\n","        return self.data_from[idx], self.data_to[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:35:16.347825Z","iopub.status.busy":"2023-03-23T08:35:16.347453Z","iopub.status.idle":"2023-03-23T08:35:16.359661Z","shell.execute_reply":"2023-03-23T08:35:16.358231Z","shell.execute_reply.started":"2023-03-23T08:35:16.347791Z"},"trusted":true},"outputs":[],"source":["def collate_fn(batch) -> tuple:\n","    batch_size = len(batch)\n","\n","    data_from = torch.Tensor([b[0].tolist() for b in batch])\n","    data_to = torch.Tensor([b[1].tolist() for b in batch])\n","\n","    distance = (torch.cdist(data_from.reshape(batch_size, -1), data_to.reshape(batch_size, -1))**2)/2\n","    distance = distance.numpy()\n","\n","    epsilon = 10\n","\n","    distance = distance/(3*32*32)\n","    epsilon = epsilon/(3*32*32)\n","\n","    a = np.ones(batch_size)/batch_size\n","    b = np.ones(batch_size)/batch_size\n","    map_probs = emd(a, b, distance)\n","\n","    to = []\n","    for b in range(batch_size):\n","        # tranposrt_idx = np.random.choice(np.arange(batch_size), size=batch_size, replace=True, p=map_probs[b+1]/(map_probs[b+1].sum()))\n","        # img = data_to[tranposrt_idx[0]]\n","\n","        tranposrt_idx = np.random.choice(np.arange(batch_size), size=1, replace=True, p=map_probs[b]/(map_probs[b].sum()))\n","        img = data_to[tranposrt_idx[0]]\n","        to.append(img.tolist())\n","    \n","    return data_from, torch.Tensor(to)\n","\n","\n","class ColoredDataset(Dataset):\n","    def __init__(self, data_from, data_to):\n","        # self.image_ids = torch.tensor(image_ids, dtype=torch.int64)\n","        self.data_from = data_from\n","        self.data_to = data_to\n","        # self.num_classes = 9\n","\n","    def __len__(self):\n","        return len(self.data_from)\n","\n","    def __getitem__(self, idx: int):\n","        \"\"\"\n","            returns single sample\n","        \"\"\"\n","        return self.data_from[idx], self.data_to[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:35:17.662388Z","iopub.status.busy":"2023-03-23T08:35:17.661369Z","iopub.status.idle":"2023-03-23T08:35:17.674161Z","shell.execute_reply":"2023-03-23T08:35:17.673052Z","shell.execute_reply.started":"2023-03-23T08:35:17.662335Z"},"id":"sVXHvOKEZ6wb","trusted":true},"outputs":[],"source":["def extract(a, t, x_shape):\n","    b, *_ = t.shape\n","    out = a.gather(-1, t)\n","    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n","\n","class GaussianDiffusion(nn.Module):\n","    def __init__(           #Remplacer par fichier config\n","        self,\n","        image_size = (224,224),\n","        channel_y = 3,\n","        channel_x = 1,\n","        timesteps = 2000\n","        ):\n","        \n","        super().__init__()\n","        \n","        self.image_size = image_size\n","        self.channel_y = channel_y\n","        self.channel_x = channel_x\n","        self.timesteps = timesteps\n","        \n","        betas = np.linspace(1e-6,0.01,timesteps)\n","        alphas = 1. - betas\n","        gammas = np.cumprod(alphas,axis=0)\n","        \n","        to_torch = partial(torch.tensor, dtype=torch.float32)\n","        \n","        #calculation for q(y_t|y_{t-1})\n","        self.register_buffer('gammas',to_torch(gammas))\n","        self.register_buffer('sqrt_one_minus_gammas',to_torch(np.sqrt(1-gammas)))\n","        self.register_buffer('sqrt_gammas',to_torch(np.sqrt(gammas)))\n","    \n","    def noisy_image(self,t,y):\n","        ''' Compute y_noisy according to (6) p15 of [2]'''\n","        noise = torch.randn_like(y)\n","        y_noisy = extract(self.gammas,t,y.shape)*y + extract(self.sqrt_one_minus_gammas,t,noise.shape)*noise\n","        return y_noisy, noise\n","        \n","    def noise_prediction(self,denoise_fn,y_noisy,x,t):\n","        ''' Use the NN to predict the noise added between y_{t-1} and y_t'''\n","        noise_pred = denoise_fn(y_noisy,x,t)\n","        return(noise_pred)"]},{"cell_type":"markdown","metadata":{"id":"SXHR7tdYatQ3"},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:35:18.799284Z","iopub.status.busy":"2023-03-23T08:35:18.798897Z","iopub.status.idle":"2023-03-23T08:35:18.875508Z","shell.execute_reply":"2023-03-23T08:35:18.874457Z","shell.execute_reply.started":"2023-03-23T08:35:18.799251Z"},"id":"bhsErMAoZ6uh","trusted":true},"outputs":[],"source":["class GroupNorm32(nn.GroupNorm):\n","    def forward(self, x):\n","        return super().forward(x.float()).type(x.dtype)\n","\n","def normalization(channels):\n","    \"\"\"\n","    Make a standard normalization layer.\n","    :param channels: number of input channels.\n","    :return: an nn.Module for normalization.\n","    \"\"\"\n","    return GroupNorm32(32, channels)\n","\n","def zero_module(module):\n","    \"\"\"\n","    Zero out the parameters of a module and return it.\n","    \"\"\"\n","    for p in module.parameters():\n","        p.detach().zero_()\n","    return module\n","\n","def checkpoint(func, inputs, params, flag):\n","    \"\"\"\n","    Evaluate a function without caching intermediate activations, allowing for\n","    reduced memory at the expense of extra compute in the backward pass.\n","    :param func: the function to evaluate.\n","    :param inputs: the argument sequence to pass to `func`.\n","    :param params: a sequence of parameters `func` depends on but does not\n","                   explicitly take as arguments.\n","    :param flag: if False, disable gradient checkpointing.\n","    \"\"\"\n","    if flag:\n","        args = tuple(inputs) + tuple(params)\n","        return CheckpointFunction.apply(func, len(inputs), *args)\n","    else:\n","        return func(*inputs)\n","\n","def timestep_embedding(timesteps, dim, max_period=10000):\n","    \"\"\"\n","    Create sinusoidal timestep embeddings.\n","    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n","                      These may be fractional.\n","    :param dim: the dimension of the output.\n","    :param max_period: controls the minimum frequency of the embeddings.\n","    :return: an [N x dim] Tensor of positional embeddings.\n","    \"\"\"\n","    half = dim // 2\n","    freqs = torch.exp(\n","        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n","    ).to(device=timesteps.device)\n","    args = timesteps[:, None].float() * freqs[None]\n","    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n","    if dim % 2:\n","        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n","    return embedding\n","\n","\n","class CheckpointFunction(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, run_function, length, *args):\n","        ctx.run_function = run_function\n","        ctx.input_tensors = list(args[:length])\n","        ctx.input_params = list(args[length:])\n","        with torch.no_grad():\n","            output_tensors = ctx.run_function(*ctx.input_tensors)\n","        return output_tensors\n","\n","    @staticmethod\n","    def backward(ctx, *output_grads):\n","        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n","        with torch.enable_grad():\n","            # Fixes a bug where the first op in run_function modifies the\n","            # Tensor storage in place, which is not allowed for detach()'d\n","            # Tensors.\n","            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n","            output_tensors = ctx.run_function(*shallow_copies)\n","        input_grads = torch.autograd.grad(\n","            output_tensors,\n","            ctx.input_tensors + ctx.input_params,\n","            output_grads,\n","            allow_unused=True,\n","        )\n","        del ctx.input_tensors\n","        del ctx.input_params\n","        del output_tensors\n","        return (None, None) + input_grads\n","\n","class EMA():\n","    def __init__(self, beta):\n","        super().__init__()\n","        self.beta = beta\n","\n","    def update_model_average(self, ma_model, current_model):\n","        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n","            old_weight, up_weight = ma_params.data, current_params.data\n","            ma_params.data = self.update_average(old_weight, up_weight)\n","\n","    def update_average(self, old, new):\n","        if old is None:\n","            return new\n","        return old * self.beta + (1 - self.beta) * new\n","\n","class TimestepBlock(nn.Module):\n","    \"\"\"\n","    Any module where forward() takes timestep embeddings as a second argument.\n","    \"\"\"\n","\n","    @abstractmethod\n","    def forward(self, x, emb):\n","        \"\"\"\n","        Apply the module to `x` given `emb` timestep embeddings.\n","        \"\"\"\n","\n","\n","class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n","    \"\"\"\n","    A sequential module that passes timestep embeddings to the children that\n","    support it as an extra input.\n","    \"\"\"\n","\n","    def forward(self, x, emb):\n","        for layer in self:\n","            if isinstance(layer, TimestepBlock):\n","                x = layer(x, emb)\n","            else:\n","                x = layer(x)\n","        return x\n","\n","class Upsample(nn.Module):\n","    \"\"\"\n","    An upsampling layer with an optional convolution.\n","    :param channels: channels in the inputs and outputs.\n","    :param use_conv: a bool determining if a convolution is applied.\n","    \"\"\"\n","\n","    def __init__(self, channels, use_conv, out_channels=None):\n","        super().__init__()\n","        self.channels = channels\n","        self.out_channels = out_channels or channels\n","        self.use_conv = use_conv\n","        if use_conv:\n","            self.conv = nn.Conv2d(self.channels, self.out_channels, 3, padding=1)\n","\n","    def forward(self, x):\n","        assert x.shape[1] == self.channels\n","        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n","        if self.use_conv:\n","            x = self.conv(x)\n","        return x\n","\n","class Downsample(nn.Module):\n","    \"\"\"\n","    A downsampling layer with an optional convolution.\n","    :param channels: channels in the inputs and outputs.\n","    :param use_conv: a bool determining if a convolution is applied.\n","    \"\"\"\n","\n","    def __init__(self, channels, use_conv, out_channels=None):\n","        super().__init__()\n","        self.channels = channels\n","        self.out_channels = out_channels or channels\n","        self.use_conv = use_conv\n","        stride = 2\n","        if use_conv:\n","            self.op = nn.Conv2d(\n","                self.channels, self.out_channels, 3, stride=stride, padding=1\n","            )\n","        else:\n","            assert self.channels == self.out_channels\n","            self.op = nn.AvgPool2d(kernel_size=stride, stride=stride)\n","\n","    def forward(self, x):\n","        assert x.shape[1] == self.channels\n","        return self.op(x)\n","\n","\n","class ResBlock(TimestepBlock):\n","    \"\"\"\n","    A residual block that can optionally change the number of channels.\n","    :param channels: the number of input channels.\n","    :param emb_channels: the number of timestep embedding channels.\n","    :param dropout: the rate of dropout.\n","    :param out_channels: if specified, the number of out channels.\n","    :param use_conv: if True and out_channels is specified, use a spatial\n","        convolution instead of a smaller 1x1 convolution to change the\n","        channels in the skip connection.\n","    :param use_checkpoint: if True, use gradient checkpointing on this module.\n","    :param up: if True, use this block for upsampling.\n","    :param down: if True, use this block for downsampling.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        channels,\n","        emb_channels,\n","        dropout,\n","        out_channels=None,\n","        use_conv=False,\n","        use_scale_shift_norm=False,\n","        use_checkpoint=False,\n","        up=False,\n","        down=False,\n","    ):\n","        super().__init__()\n","        self.channels = channels\n","        self.emb_channels = emb_channels\n","        self.dropout = dropout\n","        self.out_channels = out_channels or channels\n","        self.use_conv = use_conv\n","        self.use_checkpoint = use_checkpoint\n","        self.use_scale_shift_norm = use_scale_shift_norm\n","\n","        self.in_layers = nn.Sequential(\n","            normalization(channels),\n","            nn.SiLU(),\n","            nn.Conv2d(channels, self.out_channels, 3, padding=1),\n","        )\n","\n","        self.updown = up or down\n","\n","        if up:\n","            self.h_upd = Upsample(channels, False)\n","            self.x_upd = Upsample(channels, False)\n","        elif down:\n","            self.h_upd = Downsample(channels, False)\n","            self.x_upd = Downsample(channels, False)\n","        else:\n","            self.h_upd = self.x_upd = nn.Identity()\n","\n","        self.emb_layers = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(\n","                emb_channels,\n","                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n","            ),\n","        )\n","        self.out_layers = nn.Sequential(\n","            normalization(self.out_channels),\n","            nn.SiLU(),\n","            nn.Dropout(p=dropout),\n","            zero_module(\n","                nn.Conv2d(self.out_channels, self.out_channels, 3, padding=1)\n","            ),\n","        )\n","\n","        if self.out_channels == channels:\n","            self.skip_connection = nn.Identity()\n","        elif use_conv:\n","            self.skip_connection = nn.Conv2d(\n","                channels, self.out_channels, 3, padding=1\n","            )\n","        else:\n","            self.skip_connection = nn.Conv2d(channels, self.out_channels, 1)\n","\n","    def forward(self, x, emb):\n","        \"\"\"\n","        Apply the block to a Tensor, conditioned on a timestep embedding.\n","        :param x: an [N x C x ...] Tensor of features.\n","        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n","        :return: an [N x C x ...] Tensor of outputs.\n","        \"\"\"\n","        return checkpoint(\n","            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n","        )\n","\n","    def _forward(self, x, emb):\n","        if self.updown:\n","            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n","            h = in_rest(x)\n","            h = self.h_upd(h)\n","            x = self.x_upd(x)\n","            h = in_conv(h)\n","        else:\n","            h = self.in_layers(x)\n","        emb_out = self.emb_layers(emb).type(h.dtype)\n","        while len(emb_out.shape) < len(h.shape):\n","            emb_out = emb_out[..., None]\n","        if self.use_scale_shift_norm:\n","            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n","            scale, shift = torch.chunk(emb_out, 2, dim=1)\n","            h = out_norm(h) * (1 + scale) + shift\n","            h = out_rest(h)\n","        else:\n","            h = h + emb_out\n","            h = self.out_layers(h)\n","        return self.skip_connection(x) + h\n","\n","class AttentionBlock(nn.Module):\n","    \"\"\"\n","    An attention block that allows spatial positions to attend to each other.\n","    Originally ported from here, but adapted to the N-d case.\n","    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        channels,\n","        num_heads=1,\n","        num_head_channels=-1,\n","        use_checkpoint=False,\n","        use_new_attention_order=False,\n","    ):\n","        super().__init__()\n","        self.channels = channels\n","        if num_head_channels == -1:\n","            self.num_heads = num_heads\n","        else:\n","            assert (\n","                channels % num_head_channels == 0\n","            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n","            self.num_heads = channels // num_head_channels\n","        self.use_checkpoint = use_checkpoint\n","        self.norm = normalization(channels)\n","        self.qkv = nn.Conv1d(channels, channels * 3, 1)\n","        if use_new_attention_order:\n","            # split qkv before split heads\n","            self.attention = QKVAttention(self.num_heads)\n","        else:\n","            # split heads before split qkv\n","            self.attention = QKVAttentionLegacy(self.num_heads)\n","\n","        self.proj_out = zero_module(nn.Conv1d(channels, channels, 1))\n","\n","    def forward(self, x):\n","        return checkpoint(self._forward, (x,), self.parameters(), True)\n","\n","    def _forward(self, x):\n","        b, c, *spatial = x.shape\n","        x = x.reshape(b, c, -1)\n","        qkv = self.qkv(self.norm(x))\n","        h = self.attention(qkv)\n","        h = self.proj_out(h)\n","        return (x + h).reshape(b, c, *spatial)\n","\n","def count_flops_attn(model, _x, y):\n","    \"\"\"\n","    A counter for the `thop` package to count the operations in an\n","    attention operation.\n","    Meant to be used like:\n","        macs, params = thop.profile(\n","            model,\n","            inputs=(inputs, timestamps),\n","            custom_ops={QKVAttention: QKVAttention.count_flops},\n","        )\n","    \"\"\"\n","    b, c, *spatial = y[0].shape\n","    num_spatial = int(np.prod(spatial))\n","    # We perform two matmuls with the same number of ops.\n","    # The first computes the weight matrix, the second computes\n","    # the combination of the value vectors.\n","    matmul_ops = 2 * b * (num_spatial ** 2) * c\n","    model.total_ops += torch.DoubleTensor([matmul_ops])\n","\n","class QKVAttentionLegacy(nn.Module):\n","    \"\"\"\n","    A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\n","    \"\"\"\n","\n","    def __init__(self, n_heads):\n","        super().__init__()\n","        self.n_heads = n_heads\n","\n","    def forward(self, qkv):\n","        \"\"\"\n","        Apply QKV attention.\n","        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n","        :return: an [N x (H * C) x T] tensor after attention.\n","        \"\"\"\n","        bs, width, length = qkv.shape\n","        assert width % (3 * self.n_heads) == 0\n","        ch = width // (3 * self.n_heads)\n","        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n","        scale = 1 / math.sqrt(math.sqrt(ch))\n","        weight = torch.einsum(\n","            \"bct,bcs->bts\", q * scale, k * scale\n","        )  # More stable with f16 than dividing afterwards\n","        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n","        a = torch.einsum(\"bts,bcs->bct\", weight, v)\n","        return a.reshape(bs, -1, length)\n","\n","    @staticmethod\n","    def count_flops(model, _x, y):\n","        return count_flops_attn(model, _x, y)\n","\n","\n","class QKVAttention(nn.Module):\n","    \"\"\"\n","    A module which performs QKV attention and splits in a different order.\n","    \"\"\"\n","\n","    def __init__(self, n_heads):\n","        super().__init__()\n","        self.n_heads = n_heads\n","\n","    def forward(self, qkv):\n","        \"\"\"\n","        Apply QKV attention.\n","        :param qkv: an [N x (3 * H * C) x T] tensor of Qs, Ks, and Vs.\n","        :return: an [N x (H * C) x T] tensor after attention.\n","        \"\"\"\n","        bs, width, length = qkv.shape\n","        assert width % (3 * self.n_heads) == 0\n","        ch = width // (3 * self.n_heads)\n","        q, k, v = qkv.chunk(3, dim=1)\n","        scale = 1 / math.sqrt(math.sqrt(ch))\n","        weight = torch.einsum(\n","            \"bct,bcs->bts\",\n","            (q * scale).view(bs * self.n_heads, ch, length),\n","            (k * scale).view(bs * self.n_heads, ch, length),\n","        )  # More stable with f16 than dividing afterwards\n","        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n","        a = torch.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n","        return a.reshape(bs, -1, length)\n","\n","    @staticmethod\n","    def count_flops(model, _x, y):\n","        return count_flops_attn(model, _x, y)\n","\n","class UNetModel(nn.Module):\n","    \"\"\"\n","    The full UNet model with attention and timestep embedding.\n","    :param in_channels: channels in the input Tensor, for image colorization : Y_channels + X_channels .\n","    :param model_channels: base channel count for the model.\n","    :param out_channels: channels in the output Tensor.\n","    :param num_res_blocks: number of residual blocks per downsample.\n","    :param attention_resolutions: a collection of downsample rates at which\n","        attention will take place. May be a set, list, or tuple.\n","        For example, if this contains 4, then at 4x downsampling, attention\n","        will be used.\n","    :param dropout: the dropout probability.\n","    :param channel_mult: channel multiplier for each level of the UNet.\n","    :param conv_resample: if True, use learned convolutions for upsampling and\n","        downsampling.\n","    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n","    :param num_heads: the number of attention heads in each attention layer.\n","    :param num_heads_channels: if specified, ignore num_heads and instead use\n","                               a fixed channel width per attention head.\n","    :param num_heads_upsample: works with num_heads to set a different number\n","                               of heads for upsampling. Deprecated.\n","    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n","    :param resblock_updown: use residual blocks for up/downsampling.\n","    :param use_new_attention_order: use a different attention pattern for potentially\n","                                    increased efficiency.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        image_size,\n","        in_channels,\n","        model_channels,\n","        out_channels,\n","        num_res_blocks,\n","        attention_resolutions,\n","        dropout=0,\n","        channel_mult=(1, 2, 4, 8),\n","        conv_resample=True,\n","        use_checkpoint=False,\n","        use_fp16=False,\n","        num_heads=1,\n","        num_head_channels=-1,\n","        num_heads_upsample=-1,\n","        use_scale_shift_norm=False,\n","        resblock_updown=False,\n","        use_new_attention_order=False,\n","    ):\n","        super().__init__()\n","\n","        if num_heads_upsample == -1:\n","            num_heads_upsample = num_heads\n","\n","        self.image_size = image_size\n","        self.in_channels = in_channels\n","        self.model_channels = model_channels\n","        self.out_channels = out_channels\n","        self.num_res_blocks = num_res_blocks\n","        self.attention_resolutions = attention_resolutions\n","        self.dropout = dropout\n","        self.channel_mult = channel_mult\n","        self.conv_resample = conv_resample\n","        self.use_checkpoint = use_checkpoint\n","        self.dtype = torch.float16 if use_fp16 else torch.float32\n","        self.num_heads = num_heads\n","        self.num_head_channels = num_head_channels\n","        self.num_heads_upsample = num_heads_upsample\n","\n","        time_embed_dim = model_channels * 4\n","        self.time_embed = nn.Sequential(\n","            nn.Linear(model_channels, time_embed_dim),\n","            nn.SiLU(),\n","            nn.Linear(time_embed_dim, time_embed_dim),\n","        )\n","\n","        ch = input_ch = int(channel_mult[0] * model_channels)\n","        self.input_blocks = nn.ModuleList(\n","            [TimestepEmbedSequential(nn.Conv2d(in_channels, ch, 3, padding=1))]\n","        )\n","        self._feature_size = ch\n","        input_block_chans = [ch]\n","        ds = 1\n","        for level, mult in enumerate(channel_mult):\n","            for _ in range(num_res_blocks):\n","                layers = [\n","                    ResBlock(\n","                        ch,\n","                        time_embed_dim,\n","                        dropout,\n","                        out_channels=int(mult * model_channels),\n","                        use_checkpoint=use_checkpoint,\n","                        use_scale_shift_norm=use_scale_shift_norm,\n","                    )\n","                ]\n","                ch = int(mult * model_channels)\n","                if ds in attention_resolutions:\n","                    layers.append(\n","                        AttentionBlock(\n","                            ch,\n","                            use_checkpoint=use_checkpoint,\n","                            num_heads=num_heads,\n","                            num_head_channels=num_head_channels,\n","                            use_new_attention_order=use_new_attention_order,\n","                        )\n","                    )\n","                self.input_blocks.append(TimestepEmbedSequential(*layers))\n","                self._feature_size += ch\n","                input_block_chans.append(ch)\n","            if level != len(channel_mult) - 1:\n","                out_ch = ch\n","                self.input_blocks.append(\n","                    TimestepEmbedSequential(\n","                        ResBlock(\n","                            ch,\n","                            time_embed_dim,\n","                            dropout,\n","                            out_channels=out_ch,\n","                            use_checkpoint=use_checkpoint,\n","                            use_scale_shift_norm=use_scale_shift_norm,\n","                            down=True,\n","                        )\n","                        if resblock_updown\n","                        else Downsample(\n","                            ch, conv_resample, out_channels=out_ch\n","                        )\n","                    )\n","                )\n","                ch = out_ch\n","                input_block_chans.append(ch)\n","                ds *= 2\n","                self._feature_size += ch\n","\n","        self.middle_block = TimestepEmbedSequential(\n","            ResBlock(\n","                ch,\n","                time_embed_dim,\n","                dropout,\n","                use_checkpoint=use_checkpoint,\n","                use_scale_shift_norm=use_scale_shift_norm,\n","            ),\n","            AttentionBlock(\n","                ch,\n","                use_checkpoint=use_checkpoint,\n","                num_heads=num_heads,\n","                num_head_channels=num_head_channels,\n","                use_new_attention_order=use_new_attention_order,\n","            ),\n","            ResBlock(\n","                ch,\n","                time_embed_dim,\n","                dropout,\n","                use_checkpoint=use_checkpoint,\n","                use_scale_shift_norm=use_scale_shift_norm,\n","            ),\n","        )\n","        self._feature_size += ch\n","\n","        self.output_blocks = nn.ModuleList([])\n","        for level, mult in list(enumerate(channel_mult))[::-1]:\n","            for i in range(num_res_blocks + 1):\n","                ich = input_block_chans.pop()\n","                layers = [\n","                    ResBlock(\n","                        ch + ich,\n","                        time_embed_dim,\n","                        dropout,\n","                        out_channels=int(model_channels * mult),\n","                        use_checkpoint=use_checkpoint,\n","                        use_scale_shift_norm=use_scale_shift_norm,\n","                    )\n","                ]\n","                ch = int(model_channels * mult)\n","                if ds in attention_resolutions:\n","                    layers.append(\n","                        AttentionBlock(\n","                            ch,\n","                            use_checkpoint=use_checkpoint,\n","                            num_heads=num_heads_upsample,\n","                            num_head_channels=num_head_channels,\n","                            use_new_attention_order=use_new_attention_order,\n","                        )\n","                    )\n","                if level and i == num_res_blocks:\n","                    out_ch = ch\n","                    layers.append(\n","                        ResBlock(\n","                            ch,\n","                            time_embed_dim,\n","                            dropout,\n","                            out_channels=out_ch,\n","                            use_checkpoint=use_checkpoint,\n","                            use_scale_shift_norm=use_scale_shift_norm,\n","                            up=True,\n","                        )\n","                        if resblock_updown\n","                        else Upsample(ch, conv_resample, out_channels=out_ch)\n","                    )\n","                    ds //= 2\n","                self.output_blocks.append(TimestepEmbedSequential(*layers))\n","                self._feature_size += ch\n","\n","        self.out = nn.Sequential(\n","            normalization(ch),\n","            nn.SiLU(),\n","            zero_module(nn.Conv2d(input_ch, out_channels, 3, padding=1)),\n","        )\n","\n","    def forward(self,y, x, timesteps):\n","        \"\"\"\n","        Apply the model to an input batch.\n","        :param y: a [N x 3 x ...] Tensor of noisy colored images \n","        :param x: an [N x 1 x ...] Tensor of inputs (B&W)\n","        :param timesteps: a 1-D batch of timesteps.\n","        :return: an [N x C x ...] Tensor of outputs.\n","        \"\"\"\n","\n","        z = torch.cat([x,y],dim = 1)\n","\n","        hs = []\n","        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n","\n","        h = z.type(torch.float32)\n","        for module in self.input_blocks:\n","            h = module(h, emb)\n","            hs.append(h)\n","\n","        h = self.middle_block(h, emb)\n","\n","        for module in self.output_blocks:\n","            h = torch.cat([h, hs.pop()], dim=1)\n","            h = module(h, emb)\n","            \n","        h = h.type(z.dtype)\n","        return self.out(h)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Plottings"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:35:20.671122Z","iopub.status.busy":"2023-03-23T08:35:20.670418Z","iopub.status.idle":"2023-03-23T08:35:20.687682Z","shell.execute_reply":"2023-03-23T08:35:20.686370Z","shell.execute_reply.started":"2023-03-23T08:35:20.671079Z"},"trusted":true},"outputs":[],"source":["def fig2data ( fig ):\n","    \"\"\"\n","    @brief Convert a Matplotlib figure to a 4D numpy array with RGBA channels and return it\n","    @param fig a matplotlib figure\n","    @return a numpy 3D array of RGBA values\n","    \"\"\"\n","    # draw the renderer\n","    fig.canvas.draw ( )\n"," \n","    # Get the RGBA buffer from the figure\n","    w,h = fig.canvas.get_width_height()\n","    buf = np.fromstring ( fig.canvas.tostring_argb(), dtype=np.uint8 )\n","    buf.shape = ( w, h,4 )\n"," \n","    # canvas.tostring_argb give pixmap in ARGB mode. Roll the ALPHA channel to have it in RGBA mode\n","    buf = np.roll ( buf, 3, axis = 2 )\n","    return buf\n","\n","def fig2img ( fig ):\n","    buf = fig2data ( fig )\n","    w, h, d = buf.shape\n","    return Image.frombytes( \"RGBA\", ( w ,h ), buf.tostring( ) )\n","\n","\n","def plot_y(Grey, Y, Y_ema, Color):\n","\n","    imgs = torch.cat([Grey, Y, Y_ema, Color]).permute(0,2,3,1).mul(0.5).add(0.5).numpy().clip(0,1)\n","\n","    fig, axes = plt.subplots(4, 8, figsize=(15, 4.5), dpi=150)\n","    for i, ax in enumerate(axes.flatten()):\n","        ax.imshow(imgs[i])\n","        ax.get_xaxis().set_visible(False)\n","        ax.set_yticks([])\n","        \n","    axes[0, 0].set_ylabel('Grey', fontsize=24)\n","    axes[1, 0].set_ylabel('Y', fontsize=24)\n","    axes[2, 0].set_ylabel('Y_ema', fontsize=24)\n","    axes[3, 0].set_ylabel('Color', fontsize=24)\n","    \n","    fig.tight_layout(pad=0.001)\n","    return fig, axes\n","\n","\n","def plot_noise(ref, pred):\n","    imgs = torch.cat([ref[:10], pred[:10]]).permute(0,2,3,1).mul(0.5).add(0.5).numpy().clip(0,1)\n","\n","    fig, axes = plt.subplots(2, 10, figsize=(15, 4.5), dpi=150)\n","    for i, ax in enumerate(axes.flatten()):\n","        ax.imshow(imgs[i])\n","        ax.get_xaxis().set_visible(False)\n","        ax.set_yticks([])\n","        \n","    axes[0, 0].set_ylabel('Reference', fontsize=24)\n","    axes[1, 0].set_ylabel('Prediction', fontsize=24)\n","    \n","    fig.tight_layout(pad=0.001)\n","    return fig, axes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:35:21.726970Z","iopub.status.busy":"2023-03-23T08:35:21.725879Z","iopub.status.idle":"2023-03-23T08:35:21.733021Z","shell.execute_reply":"2023-03-23T08:35:21.731906Z","shell.execute_reply.started":"2023-03-23T08:35:21.726892Z"},"trusted":true},"outputs":[],"source":["def print_stat(st):\n","    print(f\"Min: {st.min()}, Max: {st.max()}, Mean: {st.mean()}, Std: {st.std()}\")"]},{"cell_type":"markdown","metadata":{"id":"HkpiERt5a0FA"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:35:24.442871Z","iopub.status.busy":"2023-03-23T08:35:24.442345Z","iopub.status.idle":"2023-03-23T08:35:24.482207Z","shell.execute_reply":"2023-03-23T08:35:24.481111Z","shell.execute_reply.started":"2023-03-23T08:35:24.442836Z"},"id":"cyxbKFySaMWC","trusted":true},"outputs":[],"source":["class Trainer():\n","    def __init__(self,config):\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        self.diffusion = GaussianDiffusion(config.IMAGE_SIZE,config.CHANNEL_X,config.CHANNEL_Y,config.TIMESTEPS)\n","        in_channels = config.CHANNEL_X + config.CHANNEL_Y\n","        out_channels = config.CHANNEL_Y\n","        self.network = UNetModel(\n","            config.IMAGE_SIZE,\n","            in_channels,\n","            config.MODEL_CHANNELS,\n","            out_channels,\n","            config.NUM_RESBLOCKS,\n","            config.ATTENTION_RESOLUTIONS,\n","            config.DROPOUT,\n","            config.CHANNEL_MULT,\n","            config.CONV_RESAMPLE,\n","            config.USE_CHECKPOINT,\n","            config.USE_FP16,\n","            config.NUM_HEADS,\n","            config.NUM_HEAD_CHANNELS,\n","            config.NUM_HEAD_UPSAMPLE,\n","            config.USE_SCALE_SHIFT_NORM,\n","            config.RESBLOCK_UPDOWN,\n","            config.USE_NEW_ATTENTION_ORDER,\n","            ).to(self.device)\n","\n","        train_set_bags, test_set_bags = load_shoes_bags(data_path=\"./shoes_tensor_32.torch\")\n","        train_set_shoes, test_set_shoes = load_shoes_bags(data_path=\"./handbag_tensor_32.torch\")\n","        \n","        train_dataset = ShoeBagDataset(train_set_bags, train_set_shoes)\n","        test_dataset = ShoeBagDataset(test_set_bags, test_set_shoes)\n","        \n","        self.batch_size = config.BATCH_SIZE\n","        self.batch_size_val = config.BATCH_SIZE_VAL\n","        \n","        self.dataloader_train = torch.utils.data.DataLoader(\n","                                    train_dataset,\n","                                    batch_size=self.batch_size,\n","                                    shuffle=True,\n","                                    num_workers=4,\n","                                    collate_fn=collate_fn,\n","                                    drop_last=True\n","                                )\n","\n","        self.dataloader_validation = torch.utils.data.DataLoader(\n","                                    test_dataset,\n","                                    batch_size=self.batch_size_val,\n","                                    shuffle=False,\n","                                    num_workers=4,\n","                                    collate_fn=collate_fn,\n","                                    drop_last=True\n","                                )\n","        \n","        self.iteration_max = config.ITERATION_MAX\n","        self.EMA = EMA(0.9999)\n","        self.LR = config.LR\n","        if config.LOSS == 'L1':\n","            self.loss = nn.L1Loss()\n","        if config.LOSS == 'L2':\n","            self.loss = nn.MSELoss()\n","        else :\n","            print('Loss not implemented, setting the loss to L2 (default one)')\n","        self.num_timesteps = config.TIMESTEPS\n","        self.validation_every = config.VALIDATION_EVERY\n","        self.ema_every = config.EMA_EVERY\n","        self.start_ema = config.START_EMA\n","        self.save_model_every = config.SAVE_MODEL_EVERY\n","        self.ema_model = copy.deepcopy(self.network).to(self.device)\n","        self.plot_every = config.PLOT_EVERY\n","        self.optimizer = optim.Adam(self.network.parameters(),lr=self.LR)\n","        self.iteration = 0\n","    def save_model(self,name,EMA=False):\n","        if not EMA:\n","            torch.save({\"iteration\": self.iteration,\n","                        \"model\": self.network.state_dict(),\n","                        \"optimizer\": self.optimizer.state_dict()\n","                       }, name)\n","        else:\n","            torch.save({\"iteration\": self.iteration,\n","                        \"model\": self.ema_model.state_dict(),\n","                        \"optimizer\": self.optimizer.state_dict()\n","                       }, name)\n","\n","    def train(self):\n","\n","            to_torch = partial(torch.tensor, dtype=torch.float32)\n","            \n","            print('Starting Training')\n","            \n","            wandb_step = 0\n","\n","            while self.iteration < self.iteration_max:\n","\n","                print(f\"Start of interation no. {self.iteration+1}\")\n","\n","                tq = tqdm(self.dataloader_train)\n","                \n","                for step, (grey, color) in enumerate(tq):\n","                    tq.set_description(f'Iteration {self.iteration} / {self.iteration_max}')\n","                    self.network.train()\n","                    self.optimizer.zero_grad()\n","\n","                    t = torch.randint(0, self.num_timesteps, (self.batch_size,)).long()\n","                    \n","                    noisy_image,noise_ref = self.diffusion.noisy_image(t,color)\n","                    noise_pred = self.diffusion.noise_prediction(self.network,noisy_image.to(self.device),grey.to(self.device),t.to(self.device))\n","                    loss = self.loss(noise_ref.to(self.device),noise_pred)\n","                    loss.backward()\n","                    self.optimizer.step()\n","                    tq.set_postfix(loss = loss.item())\n","\n","                    wandb.log({f'Noise_loss_train' : loss.item()}, step=wandb_step)\n","                    \n","                    self.iteration+=1\n","\n","                    if self.iteration%self.ema_every == 0 and self.iteration>self.start_ema:\n","                        print('EMA update')\n","                        self.EMA.update_model_average(self.ema_model,self.network)\n","\n","                    if self.iteration%self.plot_every == 0:\n","                        fig, ax = plot_noise(noise_ref.cpu().detach(), noise_pred.cpu().detach())\n","                        wandb.log({'Noise': [wandb.Image(fig2img(fig))]}, step=wandb_step)\n","\n","\n","                    if self.iteration%self.save_model_every == 0:\n","                        print('Saving models')\n","                        if not os.path.exists('models/'):\n","                            os.makedirs('models')\n","                        self.save_model(f'models/model_best.pth')\n","                        self.save_model(f'models/model_ema_best.pth',EMA=True)\n","\n","\n","                    wandb_step += 1\n","\n","                    if self.iteration%self.validation_every == 0:\n","                        tq_val = tqdm(self.dataloader_validation)\n","                        with torch.no_grad():\n","                            self.network.eval()\n","                            \n","                            y_mean_norms = []\n","                            for val_step, (grey, color) in enumerate(tq_val):\n","                                tq_val.set_description(f'Iteration {self.iteration} / {self.iteration_max}')\n","                                T = 1000\n","                                \n","                                betas = np.linspace(1e-6,0.01,T)\n","                                alphas = 1. - betas\n","                                gammas = to_torch(np.cumprod(alphas,axis=0))\n","                                alphas = to_torch(alphas)\n","                                betas = to_torch(betas)\n","                                \n","                                y = torch.randn_like(color)\n","                                y_norm = []\n","                                y_norm_ema = []\n","                                for t in reversed(range(T)):\n","                                    if t == 0 :\n","                                        z = torch.zeros_like(color)\n","                                    else:\n","                                        z = torch.randn_like(color)\n","\n","                                    time = (torch.ones((self.batch_size_val,)) * t).long()\n","                                    y = extract(to_torch(np.sqrt(1/alphas)),time,y.shape)*(y-(extract(to_torch((1-alphas)/np.sqrt(1-gammas)),time,y.shape))*self.network(y.to(self.device),grey.to(self.device),time.to(self.device)).detach().cpu()) + extract(to_torch(np.sqrt(1-alphas)),time,z.shape)*z\n","                                    y_ema = extract(to_torch(np.sqrt(1/alphas)),time,y.shape)*(y-(extract(to_torch((1-alphas)/np.sqrt(1-gammas)),time,y.shape))*self.ema_model(y.to(self.device),grey.to(self.device),time.to(self.device)).detach().cpu()) + extract(to_torch(np.sqrt(1-alphas)),time,z.shape)*z\n","                                    \n","\n","                                    y_mean_norms.append(torch.norm(y.reshape(y.shape[0], -1), dim=-1, p=2).mean().cpu().item())\n","                \n","                                data = [[x_coord, y_coord] for (x_coord, y_coord) in zip(range(T+1), y_mean_norms)]\n","                                table = wandb.Table(data=data, columns = [\"x\", \"y\"])\n","                                wandb.log({\"my_custom_plot_id\" : wandb.plot.line(table, \"x\", \"y\",\n","                                           title=\"Custom Y vs X Line Plot\")})\n","                \n","                                \n","                                fig, ax = plot_y(grey.cpu().detach(), y.cpu().detach(), y_ema.cpu().detach(), color.cpu().detach())\n","                                wandb.log({'Ys': [wandb.Image(fig2img(fig))]}, step=wandb_step)\n","                                    \n","\n","                                loss = self.loss(color,y)\n","                                loss_ema = self.loss(color,y_ema)\n","                                tq_val.set_postfix({'loss': loss.item(),'loss ema': loss_ema.item()})\n","\n","                                wandb.log({f'y_loss_valid': loss.item()}, step=wandb_step) \n","                                wandb.log({f'ema_loss_valid': loss_ema.item()}, step=wandb_step)\n","                                \n","                                #because validation takes too long, we simply use 1 batch\n","                                break"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Metric calculation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#get model weights\n","!kaggle kernels output kaggle3223/notebook04a4ac77c4-metrics -p ./"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def metrics(config):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    diffusion = GaussianDiffusion(config.IMAGE_SIZE,config.CHANNEL_X,config.CHANNEL_Y,config.TIMESTEPS)\n","    in_channels = config.CHANNEL_X + config.CHANNEL_Y\n","    out_channels = config.CHANNEL_Y\n","    network = UNetModel(\n","        config.IMAGE_SIZE,\n","        in_channels,\n","        config.MODEL_CHANNELS,\n","        out_channels,\n","        config.NUM_RESBLOCKS,\n","        config.ATTENTION_RESOLUTIONS,\n","        config.DROPOUT,\n","        config.CHANNEL_MULT,\n","        config.CONV_RESAMPLE,\n","        config.USE_CHECKPOINT,\n","        config.USE_FP16,\n","        config.NUM_HEADS,\n","        config.NUM_HEAD_CHANNELS,\n","        config.NUM_HEAD_UPSAMPLE,\n","        config.USE_SCALE_SHIFT_NORM,\n","        config.RESBLOCK_UPDOWN,\n","        config.USE_NEW_ATTENTION_ORDER,\n","        ).to(device)\n","\n","\n","    to_torch = partial(torch.tensor, dtype=torch.float32)\n","    batch_size_val = 64\n","\n","    _, test_set_bags = load_shoes_bags(data_path=\"./shoes_tensor_32.torch\")\n","    _, test_set_shoes = load_shoes_bags(data_path=\"./handbag_tensor_32.torch\")\n","\n","    test_dataset = ShoeBagDataset(test_set_bags, test_set_shoes)\n","\n","    dataloader_validation = torch.utils.data.DataLoader(\n","                                test_dataset,\n","                                batch_size=batch_size_val,\n","                                shuffle=False,\n","                                num_workers=4,\n","                                collate_fn=collate_fn,\n","                                drop_last=True\n","                            )\n","\n","    LR = config.LR\n","\n","    num_timesteps = config.TIMESTEPS\n","\n","    ema_model = copy.deepcopy(network).to(device)\n","\n","\n","    network.load_state_dict(torch.load(\"models/model_best.pth\")[\"model\"])\n","    ema_model.load_state_dict(torch.load(\"models/model_ema_best.pth\")[\"model\"])\n","\n","    tq_val = tqdm(dataloader_validation)\n","\n","    with torch.no_grad():\n","        network.eval()\n","        \n","        psnrs = []\n","        psnrs_ema = []\n","        ssims = []\n","        ssims_ema = []\n","        \n","        i = 0\n","        for val_step, (grey, color) in enumerate(tq_val):\n","\n","            T = 1000\n","\n","            betas = np.linspace(1e-6,0.01,T)\n","            alphas = 1. - betas\n","            gammas = to_torch(np.cumprod(alphas,axis=0))\n","            alphas = to_torch(alphas)\n","            betas = to_torch(betas)\n","\n","            y = torch.randn_like(color)\n","            y_norm = []\n","            y_norm_ema = []\n","            for t in reversed(range(T)):\n","                if t == 0 :\n","                    z = torch.zeros_like(color)\n","                else:\n","                    z = torch.randn_like(color)\n","\n","\n","                time = (torch.ones((batch_size_val,)) * t).long()\n","                y = extract(to_torch(np.sqrt(1/alphas)),time,y.shape)*(y-(extract(to_torch((1-alphas)/np.sqrt(1-gammas)),time,y.shape))*network(y.to(device),grey.to(device),time.to(device)).detach().cpu()) + extract(to_torch(np.sqrt(1-alphas)),time,z.shape)*z\n","                y_ema = extract(to_torch(np.sqrt(1/alphas)),time,y.shape)*(y-(extract(to_torch((1-alphas)/np.sqrt(1-gammas)),time,y.shape))*ema_model(y.to(device),grey.to(device),time.to(device)).detach().cpu()) + extract(to_torch(np.sqrt(1-alphas)),time,z.shape)*z\n","                \n","                \n","            psnr = PSNR(data_range=1.0)\n","            psnr.attach(default_evaluator, 'psnr')\n","            state = default_evaluator.run([[y.mul(0.5).add(0.5), color.mul(0.5).add(0.5)]])\n","            psnrs.append(state.metrics['psnr'])\n","            print(\"PSNR: \", psnrs[-1])\n","            \n","            state = default_evaluator.run([[y_ema.mul(0.5).add(0.5), color.mul(0.5).add(0.5)]])\n","            psnrs_ema.append(state.metrics['psnr'])\n","            print(\"PSNR ema: \", psnrs_ema[-1])\n","            \n","            metric = SSIM(data_range=1.0)\n","            metric.attach(default_evaluator, 'ssim')\n","            state = default_evaluator.run([[y.mul(0.5).add(0.5), color.mul(0.5).add(0.5)]])\n","            ssims.append(state.metrics['ssim'])\n","            print(\"SSIM: \", ssims[-1])\n","            \n","            state = default_evaluator.run([[y_ema.mul(0.5).add(0.5), color.mul(0.5).add(0.5)]])\n","            ssims_ema.append(state.metrics['ssim'])\n","            print(\"SSIM ema: \", ssims_ema[-1])\n","            \n","            \n","            i += 1\n","            if i == 15:\n","                break\n","                \n","    print(\"Total PSNR: \", np.mean(psnrs))\n","    print(\"Total PSNR ema: \", np.mean(psnrs_ema))\n","    print(\"Total SSIM: \", np.mean(ssims))\n","    print(\"Total SSIM ema: \", np.mean(ssims_ema))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Training/Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T08:35:26.499672Z","iopub.status.busy":"2023-03-23T08:35:26.499200Z"},"id":"kwO4WKyAKnTA","outputId":"85aa9424-569e-4412-a7f7-6b1664874e13","trusted":true},"outputs":[],"source":["import argparse\n","\n","def train(config):\n","    wandb.init(name=config.EXP_NAME, project='Diffusion_Colorization', config=config)\n","    trainer = Trainer(config)\n","    trainer.train()\n","    print('training complete')\n","\n","\n","config = load_config(\"./conf.yml\")\n","print('Config loaded')\n","mode = config.MODE\n","if mode == 1:\n","    train(config)\n","else:\n","    print(\"performing metric calculations\")\n","    metrics(config)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":4}
